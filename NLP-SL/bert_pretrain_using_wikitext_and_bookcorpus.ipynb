{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT model pretraining using Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Wikitext & Bookcorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "bookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\n",
    "wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n",
    "# wikipedia dataset only retain 'text' column\n",
    "wiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])\n",
    "\n",
    "dataset = concatenate_datasets([bookcorpus, wiki])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 80462898\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset to text\n",
    "def dataset_to_text(dataset, output_filename=\"data.txt\"):\n",
    "    with open(output_filename, \"w\") as f:\n",
    "        for t in dataset['text']:\n",
    "            print(t, file=f)\n",
    "\n",
    "# store dataset to local disk\n",
    "dataset_to_text(dataset, \"data/data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 64370318\n",
      "Test dataset size: 16092580\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "\n",
    "# Access the train and test sets\n",
    "train_dataset = train_test_split['train']\n",
    "test_dataset = train_test_split['test']\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 30_522\n",
    "max_length = 512  # max len of input sequence\n",
    "truncate_longer_samples = False  # truncate longer samples to max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 64370318\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os\n",
    "import json\n",
    "\n",
    "special_tokens = [\n",
    "    \"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"<S>\", \"<T>\"\n",
    "]\n",
    "files = [\"data/bert/data.txt\"]  # or [\"train.txt\", \"test.txt\"]\n",
    "\n",
    "tokenizer = BertWordPieceTokenizer()\n",
    "\n",
    "# train tokenizer\n",
    "tokenizer.train(files=files, vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "\n",
    "tokenizer.enable_truncation(max_length=max_length)\n",
    "\n",
    "# save tokenizer\n",
    "model_path = \"bert\"\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "tokenizer.save_model(model_path)\n",
    "# save config\n",
    "with open(os.path.join(model_path, \"config.json\"), \"w\") as f:\n",
    "    tokenizer_cfg = {\n",
    "        \"do_lower_case\": True,\n",
    "        \"unk_token\": \"[UNK]\",\n",
    "        \"sep_token\": \"[SEP]\",\n",
    "        \"pad_token\": \"[PAD]\",\n",
    "        \"cls_token\": \"[CLS]\",\n",
    "        \"mask_token\": \"[MASK]\",\n",
    "        \"max_len\": max_length,\n",
    "        \"model_max_length\": max_length\n",
    "    }\n",
    "    json.dump(tokenizer_cfg, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\"\"\"\n",
    "BertTokenizerFast is a fast version of the BertTokenizer that uses the Hugging Face tokenizers library (written in Rust) to provide much faster tokenization \n",
    "speeds compared to the regular Python-based BertTokenizer. The BertTokenizerFast is often preferred when working with large datasets due to its increased efficiency.\n",
    "\"\"\"\n",
    "model_path = \"pretrained-bert\"\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "truncate_longer_samples: False\n",
      "Encoding training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5166936/5166936 [35:25<00:00, 2430.53 examples/s]\n",
      "Map: 100%|██████████| 1291734/1291734 [08:38<00:00, 2491.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"truncate_longer_samples:\", truncate_longer_samples)\n",
    "\n",
    "def encode_with_truncation(examples):\n",
    "    \"\"\"Use tokenizer to encode examples (with truncation)\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length, return_special_tokens_mask=True)\n",
    "\n",
    "def encode_without_truncation(examples):\n",
    "    \"\"\"Use tokenizer to encode examples (no truncation)\"\"\"\n",
    "    return tokenizer(examples[\"text\"], return_special_tokens_mask=True)  # Note：no padding; sentences vary in length\n",
    "    \n",
    "\n",
    "encode = encode_with_truncation if truncate_longer_samples else encode_without_truncation\n",
    "\n",
    "\n",
    "print(\"Encoding training data...\")\n",
    "\n",
    "train_dataset_encoded = train_dataset.map(encode, batched=True)\n",
    "test_dataset_encoded = test_dataset.map(encode, batched=True)\n",
    "\n",
    "if truncate_longer_samples:\n",
    "    train_dataset_encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "    test_dataset_encoded.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "else:\n",
    "    train_dataset_encoded.set_format(columns=[\"input_ids\", \"attention_mask\", \"special_tokens_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'input_ids', 'token_type_ids', 'attention_mask', 'special_tokens_mask'],\n",
      "    num_rows: 5166936\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    if total_length >= max_length:\n",
    "        total_length = (total_length // max_length) * max_length\n",
    "    result = {\n",
    "        k: [t[i: i + max_length] for i in range(0, total_length, max_length)] for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "if not truncate_longer_samples:\n",
    "    train_dataset_encoded = train_dataset_encoded.map(group_texts, batched=True, desc=f'Grouping texts in chunks of size {max_length}')\n",
    "    test_dataset_encoded = test_dataset_encoded.map(group_texts, batched=True, desc=f'Grouping texts in chunks of size {max_length}')\n",
    "\n",
    "# 将他们从转为tensor\n",
    "train_dataset_encoded.set_format(type='torch')\n",
    "train_dataset_encoded.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERT from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM, DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
    "\n",
    "model_config = BertConfig(vocab_size=vocab_size, max_position_embeddings=max_length)\n",
    "model = BertForMaskedLM(model_config)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=32,\n",
    "    gradient_accumulation_steps=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    logging_steps=1000,\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    #load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset_encoded,\n",
    "    eval_dataset=test_dataset_encoded\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(model_path, \"checkpoint-10000\")\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    \"Today's most trending hashtages on [MASK] is Donald Trump\",\n",
    "    \"The [MASK] was cloudy yesterday, but today it's sunny!\"\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    for prediction in fill_mask(example):\n",
    "        print(f\"{prediction['sequence']}, confidence: {prediction['score']:.3f}\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
